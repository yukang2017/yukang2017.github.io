---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hi, this is Yukang Chen (é™ˆç‰åº·)â€™s website!   
I am a Research Scientist in NVIDIA, working with [Prof. Song Han](https://hanlab.mit.edu/songhan).  
I got my Ph.D. degree in CUHK, supervised by [Prof. Jiaya Jia](https://jiaya.me).  
During my Ph.D. study, I worked closely with [Prof. Xiaojuan Qi](https://scholar.google.com/citations?user=bGn0uacAAAAJ&hl=en) and [Dr. Xiangyu Zhang](https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=en).

I focus on Efficient and **Long AI** - **Boosting AI's Long ability while keeping Efficiency**, which covers:
- ğŸš— **Long-range AD**: Long-range 3D perception via **sparse convolution**.
- ğŸš€ **Long-context LLMs**: Efficient fine-tuning of long-context LLMs via **sparse attention**.
- ğŸ¥ **Long-video VLMs**: Scaling VLMs to long videos via **sequence parallelism**.
- ğŸ§  **Long-sequence Reasoning** Long-sequence RL for LLMs/VLMs via **sequence parallelism**.
- ğŸ¬ **Long-video Generation**: Shortâ†’Long AR with efficient fine-tuning via **sparse attention**.



# ğŸ”¥ News
- *2025.09*: &nbsp;ğŸ‰ğŸ‰ **[Long-RL](https://github.com/NVlabs/Long-RL)** is accepted by **Neurips'25**!
- *2025.01*: &nbsp;ğŸ‰ğŸ‰ **[LongVILA](https://arxiv.org/pdf/2408.10188)** is accepted by **ICLR'25**!
- *2024.09*: &nbsp;ğŸ‰ğŸ‰ **[RL-GPT](https://proceedings.neurips.cc/paper_files/paper/2024/file/31f119089f702e48ecfd138c1bc82c4a-Paper-Conference.pdf)** is accepted by **Neurips'24** as **Oral**!
- *2024.02*: &nbsp;ğŸ‰ğŸ‰ **[LISA](https://github.com/dvlab-research/LISA)** is accepted by **CVPR'24** as **Oral**!
- *2024.01*: &nbsp;ğŸ‰ğŸ‰ **[LongLoRA](https://github.com/dvlab-research/LongLoRA)** is accepted by **ICLR'24** as **Oral**!
- *2023.04*: &nbsp;ğŸ‰ğŸ‰ **[3D-Box-Segment-Anything](https://github.com/dvlab-research/3D-Box-Segment-Anything)** is released, a combination of [VoxelNeXt](https://github.com/dvlab-research/VoxelNeXt) and [SAM](https://arxiv.org/abs/2304.02643).
- *2023.04*: &nbsp;ğŸ‰ğŸ‰ **[VoxelNeXt](https://github.com/dvlab-research/VoxelNeXt)** is accepted by **CVPR'23**!
- *2022.03*: &nbsp;ğŸ‰ğŸ‰ **[Focal Sparse Conv](https://github.com/dvlab-research/FocalsConv)** is accepted by **CVPR'22** as **Oral**!



# ğŸ’¬ Invited Talks and Report

- *2025.07*: **[Long-RL](https://github.com/NVlabs/Long-RL)** was reported by **æœºå™¨ä¹‹å¿ƒ** with **[Link](https://www.jiqizhixin.com/articles/2025-07-14-2)**.
- *2023.10*: **[LongLoRA](https://github.com/dvlab-research/LongLoRA)** was reported by **æ–°æ™ºæº** with [Link](https://mp.weixin.qq.com/s/8QoKHgwjxv7fG_CCqouU8w).
- *2023.08*: **[LISA](https://github.com/dvlab-research/LISA)** was reported by **é‡å­ä½**, **[Link](https://mp.weixin.qq.com/s/ia7_55hfI-cs2wWalmk8yA)**.
- *2023.06*: Invited Talk by CVRP 2023 ScanNet Workshop with **[Link](http://www.scan-net.org/cvpr2023workshop/)**.
- *2023.06*: Invited Talk by VALSE 2023 Perception Workshop for **[VoxelNeXt](https://github.com/dvlab-research/VoxelNeXt)**.
- *2023.04*: Invited Talk by **å°†é—¨åˆ›æŠ•** for **[VoxelNeXt](https://github.com/dvlab-research/VoxelNeXt)**, and reported with **[Link](https://mp.weixin.qq.com/s/ijj9Zy81_645mqCaRbRFAg)**.
- *2022.06*: Invited Talk by **æ·±è“å­¦é™¢** for **[Focal Sparse Conv](https://github.com/dvlab-research/FocalsConv)**.


# ğŸ“ Publications




# ğŸ—’ï¸ Academic Services

- Conference Reviewer: Neurips, ICLR, ICML, CVPR, ICCV, ECCV, and AAAI.
- Journal Reviewer: T-PAMI and T-TIP. 
- Area Chair for AAAI 2026.


# ğŸ– Honors and Awards 

- 2025 World's Top 2% Scientists.
- 2023 Final-list candidate of ByteDance Scholarship.
- 2023 Winner of ScanNet Indoor Scene Understanding (CVPR 2023 ScanNet Workshop).
- 2022 1st of nuScenes Lidar Multi-object Tracking Leaderboard.
- 2019 Winner of COCO Detection Challenge (ICCV 2019 COCO Workshop).
